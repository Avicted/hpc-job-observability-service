services:
  # Main HPC Job Observability Service
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: hpc-observability
    ports:
      - "${PORT:-8080}:8080"
    env_file:
      - .env
    environment:
      - PORT=8080
      - HOST=0.0.0.0
      - DATABASE_URL=postgres://${POSTGRES_USER:-hpc}:${POSTGRES_PASSWORD:-hpc_password}@postgres:5432/${POSTGRES_DB:-hpc_jobs}?sslmode=disable
      - METRICS_RETENTION_DAYS=${METRICS_RETENTION_DAYS:-7}
      - SCHEDULER_BACKEND=${SCHEDULER_BACKEND:-mock}
      - SEED_DEMO=${SEED_DEMO:-false}
      # Use Docker container name for Slurm when running in Docker
      # Override SLURM_BASE_URL from .env to use container networking
      - SLURM_BASE_URL=http://slurm:6820
      - SLURM_API_VERSION=${SLURM_API_VERSION:-v0.0.36}
      - SLURM_AUTH_TOKEN=${SLURM_AUTH_TOKEN:-}
    depends_on:
      postgres:
        condition: service_healthy
    command: ["/app/server"]
    networks:
      - hpc-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/v1/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL database
  postgres:
    image: postgres:18.1-alpine
    container_name: hpc-postgres
    ports:
      - "5432:5432"
    env_file:
      - .env
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-hpc}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-hpc_password}
      - POSTGRES_DB=${POSTGRES_DB:-hpc_jobs}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - hpc-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-hpc} -d ${POSTGRES_DB:-hpc_jobs}"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Prometheus for metrics scraping
  prometheus:
    image: prom/prometheus:v3.9.1
    container_name: hpc-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    depends_on:
      - app
    networks:
      - hpc-network

  # Grafana for dashboards
  grafana:
    image: grafana/grafana:12.4.0-20648027705
    container_name: hpc-grafana
    ports:
      - "3000:3000"
    env_file:
      - .env
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD:-admin}
      - GF_AUTH_ANONYMOUS_ENABLED=${GF_AUTH_ANONYMOUS_ENABLED:-true}
      - GF_AUTH_ANONYMOUS_ORG_ROLE=${GF_AUTH_ANONYMOUS_ORG_ROLE:-Viewer}
    volumes:
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./config/grafana/provisioning/grafana-dashboard.json:/var/lib/grafana/dashboards/hpc-dashboard.json:ro
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - hpc-network

  # =============================================================================
  # Slurm Integration Testing Services (optional, use --profile slurm)
  # =============================================================================

  # Single-node Slurm cluster with slurmrestd for integration testing
  # This provides a complete Slurm environment in one container, including:
  # - slurmctld (controller)
  # - slurmd (compute daemon)
  # - slurmdbd (database daemon)  
  # - slurmrestd (REST API on port 6820)
  # - prolog/epilog scripts for job lifecycle events
  slurm:
    build:
      context: ./config/slurm
      dockerfile: Dockerfile
    container_name: hpc-slurm
    hostname: slurm
    ports:
      - "6820:6820"
    privileged: true  # Required for slurmd cgroup management
    environment:
      # Point prolog/epilog scripts to the observability service
      - OBSERVABILITY_API_URL=http://app:8080
      - OBSERVABILITY_TIMEOUT=10
    volumes:
      # Mount prolog/epilog scripts for job lifecycle events
      - ./scripts/slurm/prolog.sh:/etc/slurm/prolog.d/50-observability.sh:ro
      - ./scripts/slurm/epilog.sh:/etc/slurm/epilog.d/50-observability.sh:ro
    networks:
      - hpc-network
    profiles:
      - slurm
    # Note: No depends_on for app - Slurm starts independently.
    # The app waits for Slurm (waitForSlurmReady), avoiding circular dependency.
    # Prolog/epilog scripts handle the case where app isn't ready yet.
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:6820/openapi/v3"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s

networks:
  hpc-network:
    driver: bridge

volumes:
  postgres_data:
  prometheus_data:
  grafana_data:
